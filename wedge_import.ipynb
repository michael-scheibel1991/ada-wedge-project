{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wedge GBQ Import Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Link to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install six==1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do our imports for the code\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "my_files=os.listdir('C:\\\\Users\\\\mikes\\\\OneDrive\\\\Documents\\\\Applied Data Analytics\\\\WedgeProject2020_MikeS\\\\ada-wedge-project\\\\wedge_main_clean\\\\')\n",
    "path_to_files = 'C:\\\\Users\\\\mikes\\\\OneDrive\\\\Documents\\\\Applied Data Analytics\\\\WedgeProject2020_MikeS\\\\ada-wedge-project\\\\wedge_main_clean\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"C:\\\\Users\\\\mikes\\\\OneDrive\\\\Documents\\\\Applied Data Analytics\\\\WedgeProject2020_MikeS\\\\ada-wedge-project\\\\\"\n",
    "service_file = 'wedge2020-e941b1559b9f.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'wedge2020' # change this to your poroject. \n",
    "gbq_dataset_id = 'main_wedge_project' # and change this to your data set ID\n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Function to see if table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tbl_exists(client, table_ref):\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Code for configuration of the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.schema_update_options = [\n",
    "    bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION # This allows us to modify the table. \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Programmatic Upload\n",
    "###Creates table and loads file into GBQ for each file in the folder wedge_main_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"C:\\\\Users\\\\mikes\\\\OneDrive\\\\Documents\\\\Applied Data Analytics\\\\WedgeProject2020_MikeS\\\\ada-wedge-project\\\\wedge_main_clean\\\\\"\n",
    "\n",
    "cleaned_files = os.listdir(path_to_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transArchive_201510_clean.csv',\n",
       " 'transArchive_201511_clean.csv',\n",
       " 'transArchive_201512_clean.csv',\n",
       " 'transArchive_201601_clean.csv',\n",
       " 'transArchive_201602_clean.csv',\n",
       " 'transArchive_201603_clean.csv',\n",
       " 'transArchive_201604_clean.csv',\n",
       " 'transArchive_201605_clean.csv',\n",
       " 'transArchive_201606_clean.csv',\n",
       " 'transArchive_201607_clean.csv',\n",
       " 'transArchive_201608_clean.csv',\n",
       " 'transArchive_201609_clean.csv',\n",
       " 'transArchive_201610_clean.csv',\n",
       " 'transArchive_201611_clean.csv',\n",
       " 'transArchive_201612_clean.csv',\n",
       " 'transArchive_201701_clean.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table transArchive_201510_clean contains 50 columns\n",
      "transArchive_201510_clean.csv\n",
      "Loaded 1006055 rows into main_wedge_project:transArchive_201510_clean.\n",
      "Table transArchive_201510_clean now contains 50 columns.\n",
      "Table transArchive_201511_clean contains 0 columns\n",
      "transArchive_201511_clean.csv\n",
      "Loaded 993743 rows into main_wedge_project:transArchive_201511_clean.\n",
      "Table transArchive_201511_clean now contains 50 columns.\n",
      "Table transArchive_201512_clean contains 0 columns\n",
      "transArchive_201512_clean.csv\n",
      "Loaded 960016 rows into main_wedge_project:transArchive_201512_clean.\n",
      "Table transArchive_201512_clean now contains 50 columns.\n",
      "Table transArchive_201601_clean contains 0 columns\n",
      "transArchive_201601_clean.csv\n",
      "Loaded 979407 rows into main_wedge_project:transArchive_201601_clean.\n",
      "Table transArchive_201601_clean now contains 50 columns.\n",
      "Table transArchive_201602_clean contains 0 columns\n",
      "transArchive_201602_clean.csv\n",
      "Loaded 874852 rows into main_wedge_project:transArchive_201602_clean.\n",
      "Table transArchive_201602_clean now contains 50 columns.\n",
      "Table transArchive_201603_clean contains 0 columns\n",
      "transArchive_201603_clean.csv\n",
      "Loaded 964634 rows into main_wedge_project:transArchive_201603_clean.\n",
      "Table transArchive_201603_clean now contains 50 columns.\n",
      "Table transArchive_201604_clean contains 0 columns\n",
      "transArchive_201604_clean.csv\n",
      "Loaded 930358 rows into main_wedge_project:transArchive_201604_clean.\n",
      "Table transArchive_201604_clean now contains 50 columns.\n",
      "Table transArchive_201605_clean contains 0 columns\n",
      "transArchive_201605_clean.csv\n",
      "Loaded 938768 rows into main_wedge_project:transArchive_201605_clean.\n",
      "Table transArchive_201605_clean now contains 50 columns.\n",
      "Table transArchive_201606_clean contains 0 columns\n",
      "transArchive_201606_clean.csv\n",
      "Loaded 862328 rows into main_wedge_project:transArchive_201606_clean.\n",
      "Table transArchive_201606_clean now contains 50 columns.\n",
      "Table transArchive_201607_clean contains 0 columns\n",
      "transArchive_201607_clean.csv\n",
      "Loaded 872160 rows into main_wedge_project:transArchive_201607_clean.\n",
      "Table transArchive_201607_clean now contains 50 columns.\n",
      "Table transArchive_201608_clean contains 0 columns\n",
      "transArchive_201608_clean.csv\n",
      "Loaded 858167 rows into main_wedge_project:transArchive_201608_clean.\n",
      "Table transArchive_201608_clean now contains 50 columns.\n",
      "Table transArchive_201609_clean contains 0 columns\n",
      "transArchive_201609_clean.csv\n",
      "Loaded 861247 rows into main_wedge_project:transArchive_201609_clean.\n",
      "Table transArchive_201609_clean now contains 50 columns.\n",
      "Table transArchive_201610_clean contains 0 columns\n",
      "transArchive_201610_clean.csv\n",
      "Loaded 905091 rows into main_wedge_project:transArchive_201610_clean.\n",
      "Table transArchive_201610_clean now contains 50 columns.\n",
      "Table transArchive_201611_clean contains 0 columns\n",
      "transArchive_201611_clean.csv\n",
      "Loaded 925313 rows into main_wedge_project:transArchive_201611_clean.\n",
      "Table transArchive_201611_clean now contains 50 columns.\n",
      "Table transArchive_201612_clean contains 0 columns\n",
      "transArchive_201612_clean.csv\n",
      "Loaded 915706 rows into main_wedge_project:transArchive_201612_clean.\n",
      "Table transArchive_201612_clean now contains 50 columns.\n",
      "Table transArchive_201701_clean contains 0 columns\n",
      "transArchive_201701_clean.csv\n",
      "Loaded 936740 rows into main_wedge_project:transArchive_201701_clean.\n",
      "Table transArchive_201701_clean now contains 50 columns.\n",
      "Finished uploading tables\n"
     ]
    }
   ],
   "source": [
    "##if a table doesn't exist, uses file name to name the table for each file in the table\n",
    "for file in cleaned_files : \n",
    "    my_table, junk = file.split(\".\") #splits on period, anything after to junk, anything before goes to variable name\n",
    "    table_full_name = \".\".join([gbq_proj_id,gbq_dataset_id,my_table])\n",
    "    \n",
    "    if not tbl_exists(client, table_full_name) :\n",
    "        table_ref = client.create_table(\n",
    "            table = table_full_name\n",
    "        )\n",
    "    else :\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "        \n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} contains {} columns\".format(table_ref.table_id,len(table.schema)))\n",
    "\n",
    "#opens the file in the folder, completes the job(Loading the file) under the correct table now\n",
    "    with open(path_to_files+file, \"rb\") as source_file:\n",
    "        print(file)\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            location=\"US\", # Must match the destination dataset loaction. \n",
    "            job_config=job_config\n",
    "        ) #API \n",
    "        #break\n",
    "    job.result()  #Waits for table load to complete\n",
    "    print(\n",
    "        \"Loaded {} rows into {}:{}.\".format(\n",
    "            job.output_rows, \"main_wedge_project\", table_ref.table_id\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    table = client.get_table(table)\n",
    "    print(\"Table {} now contains {} columns.\".format(table_ref.table_id, len(table.schema)))\n",
    "\n",
    "print('Finished uploading tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
